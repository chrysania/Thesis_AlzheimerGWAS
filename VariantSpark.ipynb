{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5731af18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using variant-spark jar at '/home/jupyter-chrysanialim/.conda/envs/py37/lib/python3.7/site-packages/varspark/jars/variant-spark_2.11-0.5.0-a0-dev5-all.jar'\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/jupyter-chrysanialim/.conda/envs/py37/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/02/09 06:20:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/02/09 06:20:57 WARN Hail: This Hail JAR was compiled for Spark 3.1.1, running with Spark 3.1.3.\n",
      "  Compatibility is not guaranteed.\n",
      "Running on Apache Spark version 3.1.3\n",
      "SparkUI available at http://dto-bgsi-galaxy-dep.asia-southeast2-a.c.dto-bgsi-dev.internal:4040\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.74-0c3a74d12093\n",
      "LOGGING: writing to /home/jupyter-chrysanialim/hail-20230209-0620-0.2.74-0c3a74d12093.log\n"
     ]
    }
   ],
   "source": [
    "import hail as hl\n",
    "import varspark.hail as vshl\n",
    "vshl.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be2f16e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hail.plot import show\n",
    "from pprint import pprint\n",
    "# hl.plot.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3248ac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vcf file was converted from plink\n",
    "data = hl.import_vcf('alzheimer_vcf_2.vcf', contig_recoding={'23': 'X', '24' : 'Y', '25': 'MT'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eec3a7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    None\n",
      "----------------------------------------\n",
      "Column fields:\n",
      "    's': str\n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'locus': locus<GRCh37>\n",
      "    'alleles': array<str>\n",
      "    'rsid': str\n",
      "    'qual': float64\n",
      "    'filters': set<str>\n",
      "    'info': struct {\n",
      "        PR: bool\n",
      "    }\n",
      "----------------------------------------\n",
      "Entry fields:\n",
      "    'GT': call\n",
      "----------------------------------------\n",
      "Column key: ['s']\n",
      "Row key: ['locus', 'alleles']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c87fb221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-09 06:22:14 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'FID' as type str (user-supplied)\n",
      "  Loading field 'IID' as type str (user-supplied)\n",
      "  Loading field 'Diagnosis' as type float64 (user-supplied)\n",
      "  Loading field 'Age' as type float64 (user-supplied)\n",
      "  Loading field 'APOE' as type float64 (user-supplied)\n"
     ]
    }
   ],
   "source": [
    "# label & data format covariate file\n",
    "labels = hl.import_table('covariates_hail.csv', delimiter=';',\n",
    "                        types={'FID':'str', 'IID':'str', 'Diagnosis':'float64', 'Age':'float64', 'APOE':'float64'},\n",
    "                        key='IID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "900f1cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    None\n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'FID': str \n",
      "    'IID': str \n",
      "    'Diagnosis': float64 \n",
      "    'Age': float64 \n",
      "    'APOE': float64 \n",
      "----------------------------------------\n",
      "Key: ['IID']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "labels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d233089d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    None\n",
      "----------------------------------------\n",
      "Column fields:\n",
      "    's': str\n",
      "    'labels': struct {\n",
      "        FID: str, \n",
      "        Diagnosis: float64, \n",
      "        Age: float64, \n",
      "        APOE: float64\n",
      "    }\n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'locus': locus<GRCh37>\n",
      "    'alleles': array<str>\n",
      "    'rsid': str\n",
      "    'qual': float64\n",
      "    'filters': set<str>\n",
      "    'info': struct {\n",
      "        PR: bool\n",
      "    }\n",
      "----------------------------------------\n",
      "Entry fields:\n",
      "    'GT': call\n",
      "----------------------------------------\n",
      "Column key: ['s']\n",
      "Row key: ['locus', 'alleles']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mt = data.annotate_cols(labels = labels[data.s])\n",
    "mt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9856a762",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-09 06:22:28 Hail: INFO: Coerced sorted dataset              (3 + 1) / 4]\n",
      "[Stage 1:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(355836, 364)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt.count()\n",
    "# removed 778 SNPs in chr 3,4,19,MT due to 'not within the range of GRCh37'\n",
    "# all of chrMT SNPs are removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d9b254",
   "metadata": {},
   "source": [
    "## Logistic regression - Hail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77e27fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-09 06:27:53 Hail: INFO: Coerced sorted dataset              (3 + 1) / 4]\n"
     ]
    },
    {
     "ename": "FatalError",
     "evalue": "HailException: No complete samples: each sample is missing its phenotype or some covariate\n\nJava stack trace:\nis.hail.utils.HailException: No complete samples: each sample is missing its phenotype or some covariate\n\tat is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:11)\n\tat is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:11)\n\tat is.hail.utils.package$.fatal(package.scala:77)\n\tat is.hail.stats.RegressionUtils$.getPhenosCovCompleteSamples(RegressionUtils.scala:103)\n\tat is.hail.methods.LogisticRegression.execute(LogisticRegression.scala:37)\n\tat is.hail.expr.ir.functions.WrappedMatrixToTableFunction.execute(RelationalFunctions.scala:50)\n\tat is.hail.expr.ir.TableToTableApply.execute(TableIR.scala:2826)\n\tat is.hail.expr.ir.TableMapRows.execute(TableIR.scala:1903)\n\tat is.hail.expr.ir.Interpret$.apply(Interpret.scala:26)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$pyPersistTable$2(SparkBackend.scala:495)\n\tat is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:47)\n\tat is.hail.utils.package$.using(package.scala:627)\n\tat is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:47)\n\tat is.hail.utils.package$.using(package.scala:627)\n\tat is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17)\n\tat is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:46)\n\tat is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:275)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$pyPersistTable$1(SparkBackend.scala:494)\n\tat is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52)\n\tat is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59)\n\tat is.hail.backend.spark.SparkBackend.pyPersistTable(SparkBackend.scala:486)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n\n\nHail version: 0.2.74-0c3a74d12093\nError summary: HailException: No complete samples: each sample is missing its phenotype or some covariate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFatalError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2987173/2833345086.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                  \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_alt_alleles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                  \u001b[0mcovariates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPOE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                                  pass_through=[mt.rsid])\n\u001b[0m",
      "\u001b[0;32m<decorator-gen-1722>\u001b[0m in \u001b[0;36mlogistic_regression_rows\u001b[0;34m(test, y, x, covariates, pass_through)\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37/lib/python3.7/site-packages/hail/typecheck/check.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(__original_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m__original_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37/lib/python3.7/site-packages/hail/methods/statgen.py\u001b[0m in \u001b[0;36mlogistic_regression_rows\u001b[0;34m(test, y, x, covariates, pass_through)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransmute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogistic_regression\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-1120>\u001b[0m in \u001b[0;36mpersist\u001b[0;34m(self, storage_level)\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37/lib/python3.7/site-packages/hail/typecheck/check.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(__original_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m__original_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37/lib/python3.7/site-packages/hail/table.py\u001b[0m in \u001b[0;36mpersist\u001b[0;34m(self, storage_level)\u001b[0m\n\u001b[1;32m   1868\u001b[0m             \u001b[0mPersisted\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \"\"\"\n\u001b[0;32m-> 1870\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1872\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0munpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'Table'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37/lib/python3.7/site-packages/hail/backend/spark_backend.py\u001b[0m in \u001b[0;36mpersist_table\u001b[0;34m(self, t, storage_level)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpersist_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyPersistTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_table_ir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0munpersist_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py37/lib/python3.7/site-packages/hail/backend/py4j_backend.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 raise FatalError('%s\\n\\nJava stack trace:\\n%s\\n'\n\u001b[1;32m     31\u001b[0m                                  \u001b[0;34m'Hail version: %s\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                                  'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCapturedException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             raise FatalError('%s\\n\\nJava stack trace:\\n%s\\n'\n",
      "\u001b[0;31mFatalError\u001b[0m: HailException: No complete samples: each sample is missing its phenotype or some covariate\n\nJava stack trace:\nis.hail.utils.HailException: No complete samples: each sample is missing its phenotype or some covariate\n\tat is.hail.utils.ErrorHandling.fatal(ErrorHandling.scala:11)\n\tat is.hail.utils.ErrorHandling.fatal$(ErrorHandling.scala:11)\n\tat is.hail.utils.package$.fatal(package.scala:77)\n\tat is.hail.stats.RegressionUtils$.getPhenosCovCompleteSamples(RegressionUtils.scala:103)\n\tat is.hail.methods.LogisticRegression.execute(LogisticRegression.scala:37)\n\tat is.hail.expr.ir.functions.WrappedMatrixToTableFunction.execute(RelationalFunctions.scala:50)\n\tat is.hail.expr.ir.TableToTableApply.execute(TableIR.scala:2826)\n\tat is.hail.expr.ir.TableMapRows.execute(TableIR.scala:1903)\n\tat is.hail.expr.ir.Interpret$.apply(Interpret.scala:26)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$pyPersistTable$2(SparkBackend.scala:495)\n\tat is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:47)\n\tat is.hail.utils.package$.using(package.scala:627)\n\tat is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:47)\n\tat is.hail.utils.package$.using(package.scala:627)\n\tat is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17)\n\tat is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:46)\n\tat is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:275)\n\tat is.hail.backend.spark.SparkBackend.$anonfun$pyPersistTable$1(SparkBackend.scala:494)\n\tat is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52)\n\tat is.hail.utils.ExecutionTimer$.logTime(ExecutionTimer.scala:59)\n\tat is.hail.backend.spark.SparkBackend.pyPersistTable(SparkBackend.scala:486)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n\n\nHail version: 0.2.74-0c3a74d12093\nError summary: HailException: No complete samples: each sample is missing its phenotype or some covariate"
     ]
    }
   ],
   "source": [
    "# logistic regression: Diagnosis\n",
    "gwas_logreg_diagnosis = hl.logistic_regression_rows(test='score',\n",
    "                                y=mt.labels.Diagnosis,\n",
    "                                 x=mt.GT.n_alt_alleles(),\n",
    "                                 covariates=[1.0, mt.labels.Age, mt.labels.APOE],\n",
    "                                 pass_through=[mt.rsid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e82b6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "manhattan_logreg_diagnosis = hl.plot.manhattan(gwas_logreg_diagnosis.p_value, hover_fields=dict(rs=gwas_logreg_diagnosis.rsid))\n",
    "show(manhattan_logreg_diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268376bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hl.plot.output_notebook() # WARNING! running this will make notebook unsavable\n",
    "\n",
    "p = hl.plot.manhattan(gwas_logreg_diagnosis.p_value, hover_fields=dict(rs=gwas_logreg_diagnosis.rsid))\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce492a71",
   "metadata": {},
   "source": [
    "## Random Forest - VariantSpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42967e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = vshl.random_forest_model(y=mt.hipster.label,\n",
    "                    x=mt.GT.n_alt_alleles(), \n",
    "                    covariates={'age':mt.hipster.age, 'PC0':mt.hipster.PC0, 'PC1':mt.hipster.PC1, 'PC2':mt.hipster.PC2})\n",
    "rf_model.fit_trees(500, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f601c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture variant importances\n",
    "print(rf_model.oob_error())\n",
    "impTable = rf_model.variable_importance()\n",
    "impTable.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54b5bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the covariates importances\n",
    "covImpTable = rf_model.covariate_importance()\n",
    "covImpTable.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346edbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join hail and VariantSpark results (this is only needed here to get the RSID's)\n",
    "gwas_with_imp = gwas.join(impTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82908db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import varspark.hail.plot as vshlplt\n",
    "\n",
    "p = vshlplt.manhattan_imp(gwas_with_imp.importance, \n",
    "                            hover_fields=dict(ri=gwas_with_imp.rsid),\n",
    "                            significance_line = None)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5be71ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare logistc regression values vs. rf importance\n",
    "p = hl.plot.scatter(x=-hl.log10(gwas_with_imp.p_value),\n",
    "                    y=gwas_with_imp.importance, \n",
    "                    xlabel = '-log10(p-value)',\n",
    "                    ylabel = 'gini importance',\n",
    "                    hover_fields=dict(rs=gwas_with_imp.rsid, loc=gwas_with_imp.locus))\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e411d9",
   "metadata": {},
   "source": [
    "## Linear regression - Hail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3908cdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas = hl.linear_regression_rows(y=mt.label.score,\n",
    "                                 x=mt.GT.n_alt_alleles(),\n",
    "                                 covariates=[1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9216ae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b87f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = hl.plot.manhattan(gwas.p_value)\n",
    "show(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-py37]",
   "language": "python",
   "name": "conda-env-.conda-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
